# ğŸ­ Emotion-Aware Voice Feedback Bot

A sophisticated multi-agent system that processes voice input, detects emotions using free/open-source models, and generates empathetic AI responses through local LLM integration.

## ğŸŒŸ Features

- **ğŸ¤ Voice Input Processing** - Upload audio files or record directly
- **ğŸ“ Speech-to-Text** - Powered by OpenAI Whisper
- **ğŸ­ Advanced Emotion Recognition** - Multiple options:
  - **Hume AI Integration** (NEW!): Professional-grade emotion detection with 48+ emotions
  - **Trained RAVDESS Model**: Custom-trained model on emotional speech data
  - **Hugging Face Models**: Wav2Vec2-based emotion recognition
- **ğŸ¤– Empathetic Responses** - Generated by local Ollama LLMs
- **ğŸ’¬ Conversation History** - Track emotional patterns over time
- **ğŸ“Š Real-time Visualization** - Audio waveforms and emotion analytics
- **ğŸ”’ Privacy-First** - Local processing with optional cloud enhancement

## ğŸ—ï¸ Architecture

```
Audio Input â†’ Whisper (STT) â†’ Wav2Vec2 (Emotion) â†’ Ollama (Response) â†’ Streamlit UI
```

## ğŸš€ Quick Start

### Prerequisites

1. **Python 3.8+** installed
2. **Git** for cloning the repository
3. **FFmpeg** for audio processing
4. **Ollama** for local LLM inference

### Step 1: Install System Dependencies

#### Windows:
```bash
# Install FFmpeg using chocolatey
choco install ffmpeg

# Or download from https://ffmpeg.org/download.html
```

#### macOS:
```bash
# Install FFmpeg using homebrew
brew install ffmpeg
```

#### Linux (Ubuntu/Debian):
```bash
# Install FFmpeg
sudo apt update && sudo apt install ffmpeg
```

### Step 2: Install Ollama

#### Windows:
1. Download from [https://ollama.ai/download](https://ollama.ai/download)
2. Run the installer
3. Open Command Prompt and verify: `ollama --version`

#### macOS:
```bash
brew install ollama
```

#### Linux:
```bash
curl -fsSL https://ollama.ai/install.sh | sh
```

### Step 3: Setup the Project

```bash
# Clone the repository
git clone <your-repo-url>
cd emotion-aware-voice-feedback-bot

# Create virtual environment
python -m venv venv

# Activate virtual environment
# Windows:
venv\Scripts\activate
# macOS/Linux:
source venv/bin/activate

# Install Python dependencies
pip install -r requirements.txt
```

### Step 4: Download AI Models

```bash
# Start Ollama service (run in separate terminal)
ollama serve

# Pull required LLM models (in another terminal)
ollama pull llama2
ollama pull mistral

# Verify models are installed
ollama list
```

### Step 5: (Optional) Set Up Hume AI for Advanced Emotions

For professional-grade emotion recognition with 48+ emotions:

```bash
# Run the setup helper
python setup_hume.py

# Or manually create .env file with:
# HUME_API_KEY=your_api_key_here
# HUME_SECRET_KEY=your_secret_key_here
```

Get your API keys from [Hume AI Platform](https://platform.hume.ai/)

### Step 6: Run the Application

```bash
# Use the unified runner
python run.py

# Or run Streamlit directly
streamlit run app.py
```

The application will open in your browser at `http://localhost:8501`

## ğŸ“ Project Structure

```
emotion-aware-voice-feedback-bot/
â”œâ”€â”€ README.md                  # This file
â”œâ”€â”€ requirements.txt           # Python dependencies
â”œâ”€â”€ config.py                  # Configuration settings
â”œâ”€â”€ streamlit_app.py          # Main Streamlit application
â”œâ”€â”€ src/                      # Core source code
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ audio_utils.py        # Audio processing utilities
â”‚   â”œâ”€â”€ speech_to_text.py     # Whisper integration
â”‚   â”œâ”€â”€ emotion_recognition.py # Emotion detection
â”‚   â”œâ”€â”€ llm_response.py       # Ollama LLM integration
â”‚   â””â”€â”€ pipeline.py           # Main processing pipeline
â”œâ”€â”€ knowledge_base/           # Documentation and guides
â”œâ”€â”€ data/                     # Sample audio files (optional)
â”œâ”€â”€ models/                   # Model storage (auto-created)
â””â”€â”€ tests/                    # Unit tests (optional)
```

## ğŸ­ Emotion Recognition Options

The system supports multiple emotion recognition approaches:

### 1. Hume AI (Recommended) ğŸŒŸ
- **48+ nuanced emotions** (Joy, Frustration, Empathy, etc.)
- **Professional accuracy** from specialized AI company
- **Real-time processing** via API
- **Requires**: API key from [Hume AI](https://platform.hume.ai/)
- **Cost**: Pay-per-use API calls

### 2. Trained RAVDESS Model
- **8 basic emotions** (Happy, Sad, Angry, etc.)
- **Custom trained** on emotional speech dataset
- **Local processing** (no internet required)
- **Free** to use after training

### 3. Pretrained Models (Fallback)
- **8 basic emotions**
- **Ready to use** out of the box
- **Local processing**
- **Free** but basic accuracy

### Priority Order
1. **Hume AI** (if `HUME_API_KEY` is set)
2. **Trained Model** (if available)
3. **Pretrained** (fallback)

## ğŸ¯ Usage Guide

### Basic Usage

1. **Start the Application**
   ```bash
   streamlit run streamlit_app.py
   ```

2. **Upload Audio**
   - Click "Upload File" tab
   - Select a WAV, MP3, or M4A file
   - Audio should be 2-30 seconds for best results

3. **Process Audio**
   - Click "Analyze Emotion & Generate Response"
   - Wait for processing (first run may take longer)
   - View results: transcription, emotion, and AI response

4. **Review History**
   - Scroll down to see conversation history
   - Export data as CSV if needed

### Advanced Usage

#### Custom Model Configuration

Edit `config.py` to customize models:

```python
# Whisper model size (tiny, base, small, medium, large)
WHISPER_CONFIG = {
    "model_size": "base",  # Change to "small" for faster processing
}

# LLM configuration
LLM_CONFIG = {
    "model_name": "llama2",  # Change to "mistral" for different responses
    "temperature": 0.7,      # Adjust creativity (0.0-1.0)
}
```

#### Batch Processing

```python
from src.pipeline import get_pipeline

pipeline = get_pipeline()
audio_files = ["audio1.wav", "audio2.wav", "audio3.wav"]
results = pipeline.process_batch(audio_files)
```

## ğŸ”§ Troubleshooting

### Common Issues

#### 1. "Ollama server not found"
```bash
# Make sure Ollama is running
ollama serve

# Check if models are installed
ollama list

# Pull missing models
ollama pull llama2
```

#### 2. "Whisper model loading failed"
```bash
# Install/reinstall whisper
pip install --upgrade openai-whisper

# Clear cache and retry
rm -rf ~/.cache/whisper
```

#### 3. "Audio processing error"
```bash
# Install/reinstall audio dependencies
pip install --upgrade librosa soundfile

# Check FFmpeg installation
ffmpeg -version
```

#### 4. "Emotion model not found"
```bash
# Clear Hugging Face cache
rm -rf ~/.cache/huggingface

# Restart the application
```

### Performance Optimization

#### For Faster Processing:
1. Use smaller Whisper model: `"model_size": "tiny"`
2. Use lighter LLM: `"model_name": "mistral"`
3. Reduce audio quality before upload
4. Close other applications to free RAM

#### For Better Accuracy:
1. Use larger Whisper model: `"model_size": "large"`
2. Ensure clear audio recording
3. Speak directly into microphone
4. Minimize background noise

## ğŸ§ª Testing

### Test with Sample Audio

```bash
# Create test audio directory
mkdir data/samples

# Add sample audio files to data/samples/
# Then test with:
python -c "
from src.pipeline import process_audio_file
result = process_audio_file('data/samples/test.wav')
print(f'Emotion: {result[\"emotion\"]} - Response: {result[\"response\"]}')
"
```

### Unit Tests

```bash
# Run tests (if implemented)
python -m pytest tests/
```

## ğŸ“Š Model Information

### Speech-to-Text: OpenAI Whisper
- **Model**: Base (39M parameters)
- **Languages**: 99+ languages supported
- **Accuracy**: ~95% for clear English speech
- **Speed**: ~2-5x real-time on CPU

### Emotion Recognition: Wav2Vec2
- **Model**: ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition
- **Emotions**: Happy, Sad, Angry, Neutral, Calm, Excited, Fearful
- **Accuracy**: ~75-85% on clean speech
- **Fallback**: Rule-based detection using audio features

### Response Generation: Ollama LLMs
- **Primary**: LLaMA 2 (7B parameters)
- **Fallback**: Mistral (7B parameters)
- **Response Time**: 2-10 seconds depending on hardware
- **Context**: Maintains conversation history

## ğŸ”’ Privacy & Security

- **Local Processing**: All AI models run locally
- **No Data Upload**: Audio never leaves your machine
- **Open Source**: All models are open-source
- **Conversation History**: Stored only in session (not persistent)

## ğŸš€ Deployment Options

### Local Development
```bash
streamlit run streamlit_app.py
```

### Docker Deployment
```bash
# Build Docker image
docker build -t emotion-bot .

# Run container
docker run -p 8501:8501 emotion-bot
```

### Cloud Deployment
- **Hugging Face Spaces**: Upload code to HF Spaces
- **Streamlit Cloud**: Connect GitHub repo
- **AWS/GCP**: Use container deployment

## ğŸ¤ Contributing

1. Fork the repository
2. Create feature branch: `git checkout -b feature-name`
3. Commit changes: `git commit -am 'Add feature'`
4. Push to branch: `git push origin feature-name`
5. Submit pull request

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ™ Acknowledgments

- **OpenAI** for Whisper speech-to-text
- **Hugging Face** for emotion recognition models
- **Ollama** for local LLM inference
- **Streamlit** for the web interface
- **Meta** for LLaMA models

## ğŸ“ Support

If you encounter issues:

1. Check the troubleshooting section above
2. Review the logs in the terminal
3. Create an issue on GitHub with:
   - Error message
   - System information (OS, Python version)
   - Steps to reproduce

---

**Happy emotion-aware conversations! ğŸ­ğŸ’¬**